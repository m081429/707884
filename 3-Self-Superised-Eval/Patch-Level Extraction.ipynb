{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import umap\n",
    "import umap.plot\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.multiprocessing\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from pl_bolts.models.self_supervised import resnets\n",
    "from pl_bolts.utils.semi_supervised import Identity\n",
    "\n",
    "from DINO.vision_transformer import *\n",
    "from Classification.models.resnet_custom import resnet50_baseline\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Use\n",
    "1. Download CRC-100K, BCSS, and BreastPathQ to the \"patch_datasets\" path\n",
    "2. Place pretrain models in the \"assets_dir\" path\n",
    "3. Run the \"create_embeddings\" function to save extract features for each patch dataset\n",
    "4. Pre-extracted features for each patch dataset are available in \"embeddings_patch_library\" on Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for: Image Normalization, Patch Dataset Loading, and Patch Embedding Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_transforms(pretrained=False):\n",
    "    if pretrained:\n",
    "        mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "    else:\n",
    "        mean, std = (0.5,0.5,0.5), (0.5,0.5,0.5)\n",
    "    trnsfrms_val = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = mean, std = std)])\n",
    "    return trnsfrms_val\n",
    "\n",
    "def torchvision_ssl_encoder(name: str, pretrained: bool = False, return_all_feature_maps: bool = False):\n",
    "    pretrained_model = getattr(resnets, name)(pretrained=pretrained, return_all_feature_maps=return_all_feature_maps)\n",
    "    pretrained_model.fc = Identity()\n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):    \n",
    "    def __init__(self, dataroot, csv_path, transforms_eval):\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.csv['img_path'] = dataroot+self.csv['slide'].astype(str) + \"_\" + self.csv['rid'].astype(str) + '.tif'\n",
    "        self.transforms = transforms_eval\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.csv['img_path'][index])\n",
    "        return self.transforms(img), self.csv['y'][index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "class CSVDataset_BCSS(Dataset):    \n",
    "    def __init__(self, dataset_csv, is_train=1, transforms_eval=eval_transforms()):\n",
    "        self.csv = dataset_csv\n",
    "        self.csv = self.csv[self.csv['train']==is_train]\n",
    "        self.transforms = transforms_eval   \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.csv.index[index])\n",
    "        return self.transforms(img), self.csv.iloc[index]['label']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, fname, dataloader, dataset=None, is_imagefolder=False, \n",
    "                    save_patches=False, sprite_dim=128):\n",
    "    embeddings, labels = [], []\n",
    "    patches = []\n",
    "\n",
    "    for batch, target in tqdm(dataloader):\n",
    "        if save_patches:\n",
    "            for img in batch:\n",
    "                patches.append(tensor2im(input_image=img).resize(sprite_dim))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch = batch.to(device)\n",
    "            embeddings.append(model(batch).detach().cpu().numpy())\n",
    "            labels.append(target.numpy())\n",
    "            \n",
    "            \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.vstack(labels).squeeze()\n",
    "    \n",
    "    if is_imagefolder:\n",
    "        id2label = dict(map(reversed, dataset.class_to_idx.items()))\n",
    "        labels = np.array(list(map(id2label.get, labels.ravel())))\n",
    "\n",
    "    asset_dict = {'embeddings': embeddings, 'labels': labels}\n",
    "    if save_patches:\n",
    "        asset_dict.update({'patches': patches})\n",
    "    with open('%s.pkl' % (fname), 'wb') as handle:\n",
    "        pickle.dump(asset_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "def create_UMAP(library_path, enc_name, dataset, n=50, d=0.2):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import umap\n",
    "    \n",
    "    path = os.path.join(library_path, '%s_%s.pkl' % (dataset, enc_name))\n",
    "    with open(path, 'rb') as handle:\n",
    "        asset_dict = pickle.load(handle)\n",
    "        embeddings, labels = asset_dict['embeddings'], asset_dict['labels']\n",
    "    mapper = umap.UMAP(n_neighbors=n, min_dist=d).fit(embeddings)\n",
    "    fig = plt.figure(figsize=(10, 10), dpi=100)\n",
    "    umap.plot.points(mapper, labels=labels, width=600, height=600)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(library_path, 'UMAPs', '%s_%s_umap_n%d_d%0.2f.jpg' % (dataset, enc_name, n, d)))\n",
    "\n",
    "\n",
    "def create_embeddings(embeddings_dir, enc_name, dataset, save_patches=False, sprite_dim=128, \n",
    "                      patch_datasets='path/to/patch/datasets', assets_dir ='../ckpts/pretrain/',\n",
    "                      disentangle=-1, stage=-1):\n",
    "    print(enc_name)\n",
    "    if enc_name == 'resnet50_trunc':\n",
    "        model = resnet50_baseline(pretrained=True)\n",
    "        eval_t = eval_transforms(pretrained=True)\n",
    "        print(\"Loading ResNet50\")\n",
    "    elif 'dino' in enc_name:\n",
    "        ckpt_path = os.path.join(assets_dir, enc_name+'.pt')\n",
    "        assert os.path.isfile(ckpt_path)\n",
    "        model = vit_small(patch_size=16)\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")['teacher']\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "        print(missing_keys, unexpected_keys)\n",
    "        eval_t = eval_transforms(pretrained=False)\n",
    "        print(\"Loading DINO\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if 'dino' in enc_name:\n",
    "        _model = model\n",
    "        if stage == -1:\n",
    "            model = _model\n",
    "        else:\n",
    "            model = lambda x: torch.cat([x[:, 0] for x in _model.get_intermediate_layers(x, stage)], dim=-1)\n",
    "\n",
    "    _dis = ''\n",
    "    if stage != -1:\n",
    "        _stage = '_s%d' % stage\n",
    "    else:\n",
    "        _stage = ''\n",
    "    \n",
    "    if dataset == 'kather100k':\n",
    "        ### Train\n",
    "        dataroot = os.path.join(patch_datasets, 'NCT-CRC-HE-100K/')\n",
    "        dataset = torchvision.datasets.ImageFolder(dataroot, transform=eval_t)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=False, num_workers=4)\n",
    "        fname = os.path.join(embeddings_dir, 'kather100k_train_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=fname, dataloader=dataloader, dataset=dataset,\n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim, is_imagefolder=True)\n",
    "        \n",
    "        ### Test\n",
    "        dataroot = os.path.join(patch_datasets, 'CRC-VAL-HE-7K/')\n",
    "        dataset = torchvision.datasets.ImageFolder(dataroot, transform=eval_t)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        fname = os.path.join(embeddings_dir, 'kather100k_val_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=fname, dataloader=dataloader, dataset=dataset,\n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim, is_imagefolder=True)\n",
    "\n",
    "    elif dataset == 'kather100knonorm':\n",
    "        ### Train\n",
    "        dataroot = os.path.join(patch_datasets, 'NCT-CRC-HE-100K-NONORM/')\n",
    "        dataset = torchvision.datasets.ImageFolder(dataroot, transform=eval_t)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=False, num_workers=4)\n",
    "        fname = os.path.join(embeddings_dir, 'kather100knonorm_train_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=fname, dataloader=dataloader, dataset=dataset,\n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim, is_imagefolder=True)\n",
    "        \n",
    "        ### Test\n",
    "        dataroot = os.path.join(patch_datasets, 'CRC-VAL-HE-7K/')\n",
    "        dataset = torchvision.datasets.ImageFolder(dataroot, transform=eval_t)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        fname = os.path.join(embeddings_dir, 'kather100knonorm_val_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=fname, dataloader=dataloader, dataset=dataset,\n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim, is_imagefolder=True)\n",
    "\n",
    "    elif dataset == 'breastpathq':\n",
    "        train_dataroot = os.path.join(patch_datasets, 'BreastPathQ/breastpathq/datasets/train/')\n",
    "        val_dataroot = os.path.join(patch_datasets, 'BreastPathQ/breastpathq/datasets/validation/')\n",
    "        train_csv = os.path.join(patch_datasets, 'BreastPathQ/breastpathq/datasets/train_labels.csv')\n",
    "        val_csv = os.path.join(patch_datasets, 'BreastPathQ/breastpathq/datasets/val_labels.csv')\n",
    "\n",
    "        train_dataset = CSVDataset(dataroot=train_dataroot, csv_path=train_csv, transforms_eval=eval_t)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        val_dataset = CSVDataset(dataroot=val_dataroot, csv_path=val_csv, transforms_eval=eval_t)\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "        \n",
    "        train_fname = os.path.join(embeddings_dir, 'breastq_train_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        val_fname = os.path.join(embeddings_dir, 'breastq_val_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=train_fname, dataloader=train_dataloader, \n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim)\n",
    "        save_embeddings(model=model, fname=val_fname, dataloader=val_dataloader, \n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim)\n",
    "\n",
    "    \n",
    "    elif dataset == 'bcss':\n",
    "        dataroot = os.path.join(patch_datasets, 'BCSS/40x/patches/All/')\n",
    "        csv_path = os.path.join(patch_datasets, 'BCSS/40x/patches/summary.csv')\n",
    "        \n",
    "        dataset_csv = pd.read_csv(csv_path, sep=' ')['filename,train'].str.split(',', expand=True).astype(int)\n",
    "        dataset_csv.columns = ['label', 'train']\n",
    "        dataset_csv = dataset_csv[dataset_csv['label'].isin([0,1,2,3])]\n",
    "        dataset_csv.index = [os.path.join(dataroot, fname+'.png') for fname in dataset_csv.index]\n",
    "\n",
    "        train_dataset = CSVDataset_BCSS(dataset_csv=dataset_csv, is_train=1, transforms_eval=eval_t)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "        val_dataset = CSVDataset_BCSS(dataset_csv=dataset_csv, is_train=0, transforms_eval=eval_t)\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "        train_fname = os.path.join(embeddings_dir, 'bcss_train_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        val_fname = os.path.join(embeddings_dir, 'bcss_val_%s%s%s' % (enc_name, _dis, _stage))\n",
    "        save_embeddings(model=model, fname=train_fname, dataloader=train_dataloader, \n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim)\n",
    "        save_embeddings(model=model, fname=val_fname, dataloader=val_dataloader, \n",
    "                        save_patches=save_patches, sprite_dim=sprite_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Library Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_path = './embeddings_patch_library/'\n",
    "os.makedirs(library_path, exist_ok=True)\n",
    "\n",
    "models = ['resnet50_trunc', 'vits_tcga_brca_dino', 'vits_tcga_pancancer_dino']\n",
    "\n",
    "for enc_name in models:\n",
    "    create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='kather100knonorm')\n",
    "    if  enc_name == 'vits_tcga_pancancer_dino':\n",
    "        create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='kather100knonorm', stage=4)\n",
    "\n",
    "for enc_name in models:\n",
    "    create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='kather100k')\n",
    "    if  enc_name == 'vits_tcga_pancancer_dino':\n",
    "        create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='kather100k', stage=4)\n",
    "    \n",
    "for enc_name in models:\n",
    "    create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='bcss')\n",
    "    if  enc_name == 'vits_tcga_pancancer_dino':\n",
    "        create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='bcss', stage=4)\n",
    "    \n",
    "for enc_name in models:\n",
    "    create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='breastpathq')\n",
    "    if  enc_name == 'vits_tcga_pancancer_dino':\n",
    "        create_embeddings(embeddings_dir=library_path, enc_name=enc_name, dataset='breastpathq', stage=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
