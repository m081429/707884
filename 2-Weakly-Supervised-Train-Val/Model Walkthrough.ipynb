{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Pretraining/')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vision_transformer256 import vit_small\n",
    "from models.model_hierarchical_mil import HIPT_LGP_FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Input\n",
    "Input: $[M \\times L \\times D]$ Tensor, where:\n",
    "- M: Number of (non-overlapping) $[4096 \\times 4096]$ Image regions in a WSI (On Average: 38)\n",
    "- L: Number of (non-overlapping) $[256 \\times 256]$ Image Patches in a $[4096 \\times 4096]$ Image Region (Defaullt: 256)\n",
    "- D: Embedding Dimension (Default: 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Example Forward Pass (with Pre-Extracted $x_{256}$ Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Patches: 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fm813/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/Users/fm813/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0744,  0.3583, -0.0675, -0.0432]], grad_fn=<AddmmBackward>),\n",
       " tensor([[0.2448, 0.3252, 0.2124, 0.2176]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[1]]),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(38,256,384)\n",
    "self = HIPT_LGP_FC()\n",
    "self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward Pass Shape Walkthrough (with Pre-Extracted $x_{256}$ Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Input Tensor: torch.Size([38, 256, 384])\n",
      "\n",
      "2. Re-Arranging 1D-(Seq Length of # [256x256] tokens in [4096x4096] Region) Axis to be a 2D-Grid: torch.Size([38, 384, 16, 16])\n",
      "\n",
      "3. Seq length of [4096x4096] Tokens in the WSI torch.Size([38, 192])\n",
      "\n",
      "4. ViT-4K + Global Attention Pooling to get WSI-Level Embedding: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "x_256 = torch.randn(38,256,384)\n",
    "print(\"1. Input Tensor:\", x_256.shape)\n",
    "print()\n",
    "x_256 = x_256.unfold(1, 16, 16).transpose(1,2)\n",
    "print(\"2. Re-Arranging 1D-(Seq Length of # [256x256] tokens in [4096x4096] Region) Axis to be a 2D-Grid:\", x_256.shape)\n",
    "print()\n",
    "\n",
    "h_4096 = self.local_vit(x_256)\n",
    "print(\"3. Seq length of [4096x4096] Tokens in the WSI\", h_4096.shape)\n",
    "print()\n",
    "\n",
    "h_4096 = self.global_phi(h_4096)\n",
    "h_4096 = self.global_transformer(h_4096.unsqueeze(1)).squeeze(1)\n",
    "A_4096, h_4096 = self.global_attn_pool(h_4096)  \n",
    "A_4096 = torch.transpose(A_4096, 1, 0)\n",
    "A_4096 = F.softmax(A_4096, dim=1) \n",
    "h_path = torch.mm(A_4096, h_4096)\n",
    "h_WSI = self.global_rho(h_path)\n",
    "print(\"4. ViT-4K + Global Attention Pooling to get WSI-Level Embedding:\", h_WSI.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
